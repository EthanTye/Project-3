{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "7bba4258-ce16-47a9-ae19-8aa24ffd355c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "Fake news is a prevalent and harmful problem in modern society, often misleading the general public on important topics and polices, such as healthcare or taxes.\n",
    "\n",
    "Such misinformation can erode trust in government institutions or news agencies and result in deep seated and persistent societal issues that have a negative impact on public safety and well being.\n",
    "\n",
    "Our team aims to develop an model using Natural Language Processing and Classification Models that can accurately classify if an article contains real news or fake news based on the headline."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Background\n",
    "\n",
    "To create a model we collected the headlines of two subreddits, The Onion and Not The Onion. The onion subreddit is a collection of post that contains the headlines and links to articles produced by the satire website \"The Onion\" that publishes fictatious articles written to emulate real news articles.\n",
    "\n",
    "Not the Onion on the other hand is a subreddit that contains posts that are true but hard for readers to believe.\n",
    "\n",
    "By collecting the post data from these two subreddits we can build a collection of headlines of fake news articles and another with only real news."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part 1-Scrapping Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#importing packages required for extraction and to organize the extracted posts\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "2b86afb5-79cf-4c86-b230-8194569d49dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Scrapping data\n",
    "\n",
    "The loveshift API allows us to extract 100 post per request. For our project we plan to extract ~5000 posts per reddit. To pull the necessary number of posts, we will have to make 50 requests of 100 post each to meet our requirements. We will do this by creating a function that will make an initial request of 100 posts, after that the function will keep running the following loop until the post count reaches 5,000:\n",
    "\n",
    "(a) Make a request of 100 or the necessary number of post needed to reach 5,000 posts\n",
    "(b) drop any duplicates,\n",
    "(c) if less than 5000 posts pulled repeat.\n",
    "\n",
    "We're pulling 5,000 posts as it allows us to create a dataset with sufficient data points that will remain sizeable as we clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b16847d3-4e72-42b1-ac63-ca8ab6bfb3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating function for reddit scrapping. utc = function will only seek posts that we're created before stated utc, url = url of api, number = numnber of posts to extract\n",
    "def reddit_scrape (subreddit, utc = 1640966400,\n",
    "                   url = 'https://api.pushshift.io/reddit/search/submission',\n",
    "                   number = 5000):\n",
    "    #setting parameters to extract 1st 100 posts\n",
    "    params = {'subreddit': subreddit,\n",
    "              'size':100, #max number of posts for pullshift api\n",
    "              'before': utc}\n",
    "    res = requests.get(url,params)\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    # creating a dataframe from the posts scrapped\n",
    "    df = pd.DataFrame(posts)\n",
    "    # creating a loop where as long as the dataframe does not have 5000 posts we will keep pulling 100 posts or the number required to hit 5000 posts, whichever is less\n",
    "    while len(df)< number:\n",
    "        remainder = number-len(df)\n",
    "        # setting the size to either 100 posts or the number required to hit 5k posts\n",
    "        size = np.min((remainder, 100))\n",
    "        # parameters for requests, similar to the initial parameters however cut off date will be based on the date of the last item in the current batch of posts.\n",
    "        params_for_additions = {'subreddit': subreddit,\n",
    "                                'size': size,\n",
    "                                'before': df.created_utc.iloc[-1]}\n",
    "\n",
    "        additional_requests = requests.get(url, params_for_additions)\n",
    "        additional_data = additional_requests.json()\n",
    "        additional_posts = additional_data['data']\n",
    "        #Adding newly extracted posts to the created df\n",
    "        df = pd.concat([df, pd.DataFrame(additional_posts)], axis=0)\n",
    "        #dropping any duplicates\n",
    "        df.drop_duplicates(subset = ['title'], keep = 'last', inplace = True)\n",
    "        #resetting index\n",
    "        df.reset_index(inplace = True, drop = True)\n",
    "    #Once 5000 unique posts have been created, save data into a csv\n",
    "    df.to_csv(f'../Datasets/{subreddit}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6356d8d0-c87a-4dac-9c8e-ff2633fee27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the function to extract posts from \"TheOnion\"\n",
    "reddit_scrape('theonion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38f0d5c1-ced9-43b2-b537-b49dfbb383f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#running the function to extract posts from \"Nottheonion\"\n",
    "reddit_scrape('nottheonion')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we'll read in the csv files with the posts details from \"the onion\" and \"nottheonion\" so that we can review the contents to ensure that the function extracted the post details as intended."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af41a24-6999-40ff-b819-0c140a473edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the datasets\n",
    "df_notonion = pd.read_csv('../Datasets/nottheonion.csv', index_col = [0])\n",
    "df_onion = pd.read_csv('../Datasets/theonion.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d5a8cf-fc0f-40fc-8c0d-73e27be082ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5000 entries, 0 to 4999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   5000 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 78.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#review contents\n",
    "df_notonion[['title']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5000 entries, 0 to 4999\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   5000 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 78.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_onion[['title']].info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2acf591a-e141-46db-b005-c24a7e403bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for duplicates\n",
    "df_onion[df_onion['title'].duplicated()==True]['title'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notonion[df_notonion['title'].duplicated()== True]['title'].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At first glance there doesn't seem to be any duplicates in our data and there are no nulls in the title field."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f063fbe-643f-43d5-b98c-67e2fff07648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the 2 datasets together\n",
    "df = df_onion.append(df_notonion, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True, drop = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the purpose of out analysis we will be using just the title of the post to determine which subreddit it belongs to. As such we will only need to keep the 'title' and 'subreddit' columns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Saving selected features into a csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bee4ac8-58c0-40ec-9367-f686b2cb9802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#selecting only title and subreddit columns\n",
    "df = df[['subreddit', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b152daba-4ae9-4a4d-85c4-5545fdc64c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the combined items into a csv\n",
    "df.to_csv('../Datasets/combined.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "#end of book 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}